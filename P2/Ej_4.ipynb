{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ej. 4) Idem al problema anterior pero ahora que la función de costo sea Categorical Cross-Entropy.\\\n",
    "Esta función de costo puede verse, al igual que para el clasificador lineal SoftMax, como\\\n",
    "aplicar la función softmax seguido de la log-likelihood. Modificar el código anterior para\\\n",
    "que se pueda cambiar la métrica a utilizar (loss_mse o loss_softmax) y lo mismo para el\\\n",
    "cálculo del gradiente (grad_mse o grad_softmax). Analizar los resultados respecto a los\\\n",
    "clasificadores lineales SVM y Softamx como también con la resolución del punto anterior.\\\n",
    "Armar un gráfico aparte para mostrar la precisión de estos cuatro métodos todos juntos\\\n",
    "(sólo para los datos de testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ej. 5) Idem al problema anterior pero esta vez que las funciones de activación de la capa oculta\\\n",
    "sean ReLU y las de la capa de salida sean sigmoidal. Al igual que en los problemas anteriores\\\n",
    "armar el grafo, calcular los gradientes, graficar la evolución de la precisión (accuracy) y las\\\n",
    "funciones de costo (mse y categorical cross-entropy) para las distintas épocas tanto para los\\\n",
    "datos de validación como entrenamiento. Discuta que métrica es mejor y porque. Además\\\n",
    "analizar qué pasa si usamos una ReLU + activación lineal en la última capa. Explicar y\\\n",
    "fundamentar si usar una activación lineal en la última capa es mejor o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg\n",
    "from scipy.special import expit\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_dict = {0: 'airplane', 1: 'automobile', 2: 'bird', \n",
    "              3: 'cat', 4: 'deer', 5: 'dog', \n",
    "              6: 'frog', 7: 'horse', 8: 'ship', \n",
    "              9: 'truck'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(X):\n",
    "        X_norm = X.reshape(len(X), X[0].size).astype(float)\n",
    "        X_norm -= np.mean(X)\n",
    "        X_norm /= np.std(X).astype(float)\n",
    "        return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return expit(z)\n",
    "\n",
    "def sigprime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(ypred, ytrue):\n",
    "    ypred_idx = np.argmax(ypred, axis=1)\n",
    "\n",
    "    return np.mean(ypred_idx==ytrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(yb_pred, yb):\n",
    "    diff = np.copy(yb_pred)\n",
    "    m = yb_pred.shape[0]\n",
    "    idx = np.arange(0, m)\n",
    "    diff[idx, yb] -= 1\n",
    "\n",
    "    return (linalg.norm(diff)**2 / (2*m))\n",
    "\n",
    "def grad_MSE(yb_pred, yb):\n",
    "    diff = np.copy(yb_pred)\n",
    "    m = yb_pred.shape[0]\n",
    "    idx = np.arange(0, m)\n",
    "    diff[idx, yb] -= 1\n",
    "    \n",
    "    return diff/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![backprop](backprop.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(Xtrain, ytrain, Xtest, ytest, batch_size, metric='MSE', epochs=10, alpha=.005, lambda_= .01):\n",
    "    #Calculo tamaño del batch\n",
    "    #Inicializo variables etc\n",
    "    m = Xtrain.shape[0]\n",
    "    n = Xtrain.shape[1]\n",
    "    n_batches = int(m/batch_size)\n",
    "    grad_metric = \"grad_\"+metric\n",
    "\n",
    "    Xtrain = np.hstack((np.ones((m, 1)), Xtrain))\n",
    "    Xtest = np.hstack((np.ones((Xtest.shape[0], 1)), Xtest))\n",
    "\n",
    "    Xb = np.zeros((batch_size, n), dtype=float)\n",
    "    Xb = np.hstack((np.ones((batch_size, 1)), Xb))\n",
    "    yb = np.zeros((batch_size), dtype=float)\n",
    "    W1 = np.random.uniform(-1e-2, 1e-2, size=(n+1, 100))\n",
    "    W2 = np.random.uniform(-1e-2, 1e-2, size=(101, 10))\n",
    "\n",
    "    loss_train = []\n",
    "    acc_train = []\n",
    "    loss_test = []\n",
    "    acc_test = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        #Shuffle indice datos de training\n",
    "        shuffle = np.random.permutation(m)\n",
    "\n",
    "        Xtrain = Xtrain[shuffle]\n",
    "        ytrain = ytrain[shuffle]\n",
    "        #Calculo terminos de regularizacion\n",
    "        L2 = linalg.norm(W1)**2 + linalg.norm(W2)**2\n",
    "\n",
    "        for batch in range(n_batches):\n",
    "            Xb = Xtrain[batch*batch_size : (batch+1)*batch_size, :]\n",
    "            yb = ytrain[batch*batch_size : (batch+1)*batch_size]\n",
    "            \n",
    "            #Forward todo mi batch\n",
    "            z1 = sigmoid(np.dot(Xb, W1))\n",
    "            z1 = np.hstack((np.ones((batch_size, 1)), z1))\n",
    "            yb_pred = np.dot(z1, W2)\n",
    "\n",
    "            #Calculo loss con la metrica elegida\n",
    "            loss = globals()[metric](yb_pred, yb) + (lambda_/2) * L2\n",
    "\n",
    "            #Calculo grads:\n",
    "            #grad de la funcion de loss\n",
    "            grad = globals()[grad_metric](yb_pred, yb)\n",
    "\n",
    "            #Layer 2\n",
    "            gradW2 = np.dot(z1.T, grad) #S1 tiene la columna de unos\n",
    "            grad = np.dot(grad, W2.T)\n",
    "            grad = grad[:, 1:]  \n",
    "\n",
    "            #Layer 1\n",
    "            grad_sig = sigprime(np.dot(Xb, W1))\n",
    "            grad = grad * grad_sig  #Producto elemento a elemento\n",
    "            gradW1 = np.dot(Xb.T, grad) #Xb incluye columna de 1s\n",
    "            \n",
    "            W1 -= alpha*(gradW1 + lambda_ * W1)\n",
    "            W2 -= alpha*(gradW2 + lambda_ * W2)\n",
    "        \n",
    "        #Calculo loss y accuracy\n",
    "        loss_train.append(loss)\n",
    "        acc_train.append(accuracy(yb_pred, yb))\n",
    "        \n",
    "        z1t = sigmoid(np.dot(Xtest, W1))\n",
    "        z1t = np.hstack((np.ones((Xtest.shape[0], 1)), z1t))\n",
    "        ybt_pred = np.dot(z1t, W2)\n",
    "        loss_test.append(globals()[metric](ybt_pred, ytest) + (lambda_/2) * L2)\n",
    "        acc_test.append(accuracy(ybt_pred, ytest))\n",
    "        \n",
    "        #if epoch%10 == 0:\n",
    "        print(\"Train loss of epoch {}: {:.6f}, Train accuracy: {:.2f}%\".format(epoch, loss_train[-1], 100*acc_train[-1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(50000, 3072), y=(50000,)\n",
      "Test: X=(10000, 3072), y=(10000,)\n"
     ]
    }
   ],
   "source": [
    "(Xtrain, ytrain), (Xtest, ytest) = cifar10.load_data()\n",
    "\n",
    "ytrain = ytrain[:,0]\n",
    "ytest = ytest[:,0]\n",
    "\n",
    "Xtrain = clean(Xtrain)\n",
    "Xtest = clean(Xtest)\n",
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (Xtrain.shape, ytrain.shape))\n",
    "print('Test: X=%s, y=%s' % (Xtest.shape, ytest.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss of epoch 0: 0.499094, Train accuracy: 18.70%\n",
      "Train loss of epoch 1: 0.496678, Train accuracy: 22.70%\n",
      "Train loss of epoch 2: 0.495410, Train accuracy: 21.80%\n",
      "Train loss of epoch 3: 0.491833, Train accuracy: 24.90%\n",
      "Train loss of epoch 4: 0.489357, Train accuracy: 22.30%\n",
      "Train loss of epoch 5: 0.487061, Train accuracy: 24.50%\n",
      "Train loss of epoch 6: 0.485620, Train accuracy: 26.80%\n",
      "Train loss of epoch 7: 0.482329, Train accuracy: 26.90%\n",
      "Train loss of epoch 8: 0.481638, Train accuracy: 26.20%\n",
      "Train loss of epoch 9: 0.480182, Train accuracy: 27.20%\n",
      "Train loss of epoch 10: 0.479017, Train accuracy: 26.10%\n",
      "Train loss of epoch 11: 0.478388, Train accuracy: 25.50%\n",
      "Train loss of epoch 12: 0.475472, Train accuracy: 27.20%\n",
      "Train loss of epoch 13: 0.474844, Train accuracy: 25.60%\n",
      "Train loss of epoch 14: 0.471884, Train accuracy: 31.00%\n",
      "Train loss of epoch 15: 0.470429, Train accuracy: 28.30%\n",
      "Train loss of epoch 16: 0.467066, Train accuracy: 29.60%\n",
      "Train loss of epoch 17: 0.469791, Train accuracy: 28.40%\n",
      "Train loss of epoch 18: 0.469200, Train accuracy: 27.30%\n",
      "Train loss of epoch 19: 0.465563, Train accuracy: 29.30%\n",
      "Train loss of epoch 20: 0.470388, Train accuracy: 27.90%\n",
      "Train loss of epoch 21: 0.465734, Train accuracy: 30.30%\n",
      "Train loss of epoch 22: 0.467455, Train accuracy: 28.10%\n",
      "Train loss of epoch 23: 0.465274, Train accuracy: 29.60%\n",
      "Train loss of epoch 24: 0.463191, Train accuracy: 30.20%\n",
      "Train loss of epoch 25: 0.464811, Train accuracy: 31.00%\n",
      "Train loss of epoch 26: 0.463287, Train accuracy: 29.90%\n",
      "Train loss of epoch 27: 0.462863, Train accuracy: 30.80%\n",
      "Train loss of epoch 28: 0.464812, Train accuracy: 31.20%\n",
      "Train loss of epoch 29: 0.462862, Train accuracy: 31.60%\n",
      "Train loss of epoch 30: 0.461402, Train accuracy: 33.30%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Lautaro\\Desktop\\Lautaro\\asd\\2021\\Doctorado\\Materias\\Deep Learning (2020)\\Practicas\\Mias\\P2\\Ej_4.ipynb Celda 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Lautaro/Desktop/Lautaro/asd/2021/Doctorado/Materias/Deep%20Learning%20%282020%29/Practicas/Mias/P2/Ej_4.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m fit(Xtrain, ytrain, Xtest, ytest, batch_size\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, metric\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mMSE\u001b[39;49m\u001b[39m\"\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Lautaro\\Desktop\\Lautaro\\asd\\2021\\Doctorado\\Materias\\Deep Learning (2020)\\Practicas\\Mias\\P2\\Ej_4.ipynb Celda 12\u001b[0m in \u001b[0;36mfit\u001b[1;34m(Xtrain, ytrain, Xtest, ytest, batch_size, metric, epochs, alpha, lambda_)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lautaro/Desktop/Lautaro/asd/2021/Doctorado/Materias/Deep%20Learning%20%282020%29/Practicas/Mias/P2/Ej_4.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m yb \u001b[39m=\u001b[39m ytrain[batch\u001b[39m*\u001b[39mbatch_size : (batch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mbatch_size]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lautaro/Desktop/Lautaro/asd/2021/Doctorado/Materias/Deep%20Learning%20%282020%29/Practicas/Mias/P2/Ej_4.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m#Forward todo mi batch\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Lautaro/Desktop/Lautaro/asd/2021/Doctorado/Materias/Deep%20Learning%20%282020%29/Practicas/Mias/P2/Ej_4.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m z1 \u001b[39m=\u001b[39m sigmoid(np\u001b[39m.\u001b[39;49mdot(Xb, W1))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lautaro/Desktop/Lautaro/asd/2021/Doctorado/Materias/Deep%20Learning%20%282020%29/Practicas/Mias/P2/Ej_4.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m z1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack((np\u001b[39m.\u001b[39mones((batch_size, \u001b[39m1\u001b[39m)), z1))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lautaro/Desktop/Lautaro/asd/2021/Doctorado/Materias/Deep%20Learning%20%282020%29/Practicas/Mias/P2/Ej_4.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m yb_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(z1, W2)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit(Xtrain, ytrain, Xtest, ytest, batch_size=1000, metric=\"MSE\", epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b045fddb768a69d708aeb44bac4815a8765073d6505277d1278f194b80f63e22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
